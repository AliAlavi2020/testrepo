{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM86UjT2qqDxjw3RXpjZczo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliAlavi2020/testrepo/blob/main/Intent_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tuning a Pre-Trained BERT Model for Intent Classification2\n",
        "\n",
        "This code fine-tunes a pre-trained BERT model, specifically the ParsBert model, for intent classification on a custom dataset. The dataset consists of text samples with corresponding labels, where each label represents a specific intent.\n",
        "\n",
        "The code defines a custom dataset class IntentDataset to load and preprocess the data, and a custom classifier model ParsBertClassifier that builds upon the pre-trained BERT model. The classifier model adds a dropout layer and a linear layer on top of the BERT model to output probabilities for each intent class.\n",
        "\n",
        "The code then loads the pre-trained BERT model and tokenizer, creates a dataset and data loader for the training data, and defines the training parameters, including the optimizer and learning rate.\n",
        "\n",
        "The model is trained for 3 epochs using the Adam optimizer and cross-entropy loss. The model is evaluated on the training data after each epoch, and the loss is printed to track the training progress.\n",
        "\n",
        "After training, the model is fine-tuned and ready to be used for intent classification on new, unseen data."
      ],
      "metadata": {
        "id": "34D-QWSDVYK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU hazm"
      ],
      "metadata": {
        "id": "xcB3DjZNVOSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBeZdkdRHosU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d60a551-a3ab-4725-b0be-42f0269ae18a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.8195169866085052\n",
            "Epoch 2, Loss: 0.5882050693035126\n",
            "Epoch 3, Loss: 0.40464162826538086\n",
            "Model fine-tuned successfully.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "\n",
        "\n",
        "\n",
        "def parsbert_ner_load_model(model_name):\n",
        "    \"\"\"Load the model\"\"\"\n",
        "    try:\n",
        "        config = AutoConfig.from_pretrained(model_name)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = TFAutoModelForTokenClassification.from_pretrained(model_name)\n",
        "        labels = list(config.label2id.keys())\n",
        "\n",
        "        return model, tokenizer, labels\n",
        "    except:\n",
        "        return [None] * 3\n",
        "\n",
        "# Define a custom dataset for loading data\n",
        "class IntentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        ###self.tokenizer = ParsBertTokenizer.from_pretrained('parsbert')\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class ParsBertClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ParsBertClassifier, self).__init__()\n",
        "        self.bert = model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        outputs = self.classifier(pooled_output)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "# Load tokenizer and data\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "train_texts = [\"سفارش من ارسال شده است؟\", \"تقاضای عودت سفارش را دارم\",\"بازگشت محصول \"]\n",
        "train_labels = [0, 1, 1]  # Assume 0: GetOrderStatus, 1: RequestRefund\n",
        "\n",
        "train_dataset = IntentDataset(train_texts, train_labels, tokenizer, max_len=32)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Load pretrained model\n",
        "model = AutoModel.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "\n",
        "num_classes = 2  # Replace with the actual number of classes\n",
        "model = ParsBertClassifier(num_classes)\n",
        "\n",
        "\n",
        "# Define training parameters\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)\n",
        "###model.train()\n",
        "\n",
        "\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['label']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n",
        "\n",
        "\n",
        "print(\"Model fine-tuned successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "id": "pkmVJ-CVVHGv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fec9de0-daae-4668-b39d-e3c8254c2918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Selection and Training\n",
        "\n",
        "Select a Pretrained Model: Use a pretrained model like BERT, RoBERTa, or DistilBERT from the Hugging Face Transformers library.\n",
        "Fine-Tune the Model: Fine-tune the pretrained model on your labeled dataset to adapt it to your specific intents.\n",
        "Example code using Hugging Face Transformers and PyTorch:"
      ],
      "metadata": {
        "id": "4dzTf6GFID89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference\n",
        "\n",
        "Predict the Intent: Once the model is trained, use it to predict the intent of new user queries."
      ],
      "metadata": {
        "id": "uXl-6c69IM2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_intent(model, tokenizer, text):\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=32,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "    input_ids = encoding['input_ids']\n",
        "    attention_mask = encoding['attention_mask']\n",
        "\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    _, prediction = torch.max(outputs, dim=1)\n",
        "\n",
        "    return prediction.item()\n",
        "\n",
        "#user_query = \"وضعیت سفارش من چگونه است؟\"\n",
        "user_query =\"چگونه کالا را بازگشت بدهم؟\"\n",
        "#user_query =\"چگونه کالا را برگردانم؟\"\n",
        "#user_query =\"سفارش من چه زمانی به دست من میرسد؟\"\n",
        "#user_query =\"چگون با واحد پشتیبانی صحبت کنم\"\n",
        "intent_id = predict_intent(model, tokenizer, user_query)\n",
        "intents = {0: \"GetOrderStatus\", 1: \"RequestRefund\"}\n",
        "print(f'Intent: {intents[intent_id]}')\n"
      ],
      "metadata": {
        "id": "RzifE9j5IS_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee7377e-f62a-4772-ccd6-5cd16ceb0956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent: GetOrderStatus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entity Extraction\n",
        "What is Entity Extraction?\n",
        "Entity extraction (or named entity recognition, NER) involves identifying and classifying key pieces of information (entities) within a user’s input. For example, in “Track order 12345”, “12345” would be the entity recognized as an OrderID.\n",
        "\n",
        "Steps to Implement Entity Extraction\n",
        "Data Preparation\n",
        "\n",
        "Collect and Label Data: Annotate training sentences with entities. For example:\n",
        "Sentence: \"Track order 12345\"\n",
        "Entities: [\"OrderID\": \"12345\"]"
      ],
      "metadata": {
        "id": "nklWtVBtIqut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Selection and Training\n",
        "\n",
        "Select a Pretrained NER Model: Use models like BERT or SpaCy pretrained NER models from Hugging Face.\n",
        "Fine-Tune: Fine-tune the model on your custom dataset if specific entity types are needed.\n",
        "Example using Hugging Face Transformers with a pretrained token classification model:"
      ],
      "metadata": {
        "id": "ZhQrFJr7Iy4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_order_number(text):\n",
        "    pattern = r'\\b\\d{5}\\b'\n",
        "    match = re.search(pattern, text)\n",
        "    if match:\n",
        "        return match.group()\n",
        "    else:\n",
        "        return \"Invalid order number. Please enter a 5-figure number.\"\n",
        "\n",
        "# Test the function\n",
        "text = \"کد رهگیری سفارش من برای خرید یک دستگاه قهوه ساز 52374 است \"\n",
        "# texts = [\n",
        "#    \"مدیرکل محیط زیست استان البرز با بیان اینکه با بیان اینکه موضوع شیرابه‌های زباله‌های انتقال یافته در منطقه حلقه دره خطری برای این استان است، گفت: در این مورد گزارشاتی در ۲۵ مرداد ۱۳۹۷ تقدیم مدیران استان شده است.\",\n",
        "#    \"به گزارش خبرگزاری تسنیم از کرج، حسین محمدی در نشست خبری مشترک با معاون خدمات شهری شهرداری کرج که با حضور مدیرعامل سازمان‌های پسماند، پارک‌ها و فضای سبز و نماینده منابع طبیعی در سالن کنفرانس شهرداری کرج برگزار شد، اظهار داشت: ۸۰٪  جمعیت استان البرز در کلانشهر کرج زندگی می‌کنند.\",\n",
        "#    \"وی افزود: با همکاری‌های مشترک بین اداره کل محیط زیست و شهرداری کرج برنامه‌های مشترکی برای حفاظت از محیط زیست در شهر کرج در دستور کار قرار گرفته که این اقدامات آثار مثبتی داشته و تاکنون نزدیک به ۱۰۰ میلیارد هزینه جهت خریداری اکس-ریس شیراز صورت گرفته است.\",\n",
        "# ]\n",
        "order_number = extract_order_number(text)\n",
        "print(order_number)"
      ],
      "metadata": {
        "id": "CoegNZ0aRKxn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3548d50f-d972-4dda-a81a-5585ac8629a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hazm\n",
        "from IPython.display import HTML\n",
        "\n",
        "ner_translate = {\n",
        "    \"B-date\": \"تاریخ\",\n",
        "    \"B-event\": \"رویداد\",\n",
        "    \"B-facility\": \"امکانات\",\n",
        "    \"B-location\": \"موقعیت\",\n",
        "    \"B-money\": \"پول\",\n",
        "    \"B-organization\": \"سازمان\",\n",
        "    \"B-person\": \"شخص\",\n",
        "    \"B-product\": \"محصول\",\n",
        "    \"B-time\": \"زمان\",\n",
        "    \"B-percent\": \"درصد\",\n",
        "    \"I-date\": \"تاریخ\",\n",
        "    \"I-event\": \"رویداد\",\n",
        "    \"I-facility\": \"امکانات\",\n",
        "    \"I-location\": \"موقعیت\",\n",
        "    \"I-money\": \"پول\",\n",
        "    \"I-organization\": \"سازمان\",\n",
        "    \"I-person\": \"شخص\",\n",
        "    \"I-product\": \"محصول\",\n",
        "    \"I-time\": \"زمان\",\n",
        "    \"I-percent\": \"درصد\",\n",
        "    \"O\": None\n",
        "}\n",
        "\n",
        "normalizer = hazm.Normalizer()\n",
        "\n",
        "def cleanize(text):\n",
        "    \"\"\"A way to normalize and even clean the text\"\"\"\n",
        "    # clean text\n",
        "    # do some fns\n",
        "    return normalizer.normalize(text)\n",
        "\n",
        "def parsbert_ner_load_model(model_name):\n",
        "    \"\"\"Load the model\"\"\"\n",
        "    try:\n",
        "        config = AutoConfig.from_pretrained(model_name)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = TFAutoModelForTokenClassification.from_pretrained(model_name)\n",
        "        labels = list(config.label2id.keys())\n",
        "\n",
        "        return model, tokenizer, labels\n",
        "    except:\n",
        "        return [None] * 3\n",
        "\n",
        "def parsbert_ner(texts, model_name, label_translate, visualize=True):\n",
        "    \"\"\"Predict and visualize the NER!\"\"\"\n",
        "    global css_is_load\n",
        "\n",
        "    css_is_load = False\n",
        "    css = \"\"\"<style>\n",
        "    .ner-box {\n",
        "        direction: rtl;\n",
        "        font-size: 18px !important;\n",
        "        line-height: 20px !important;\n",
        "        margin: 0 0 15px;\n",
        "        padding: 10px;\n",
        "        text-align: justify;\n",
        "        color: #343434 !important;\n",
        "    }\n",
        "    .token, .token span {\n",
        "        display: inline-block !important;\n",
        "        padding: 2px;\n",
        "        margin: 2px 0;\n",
        "    }\n",
        "    .token.token-ner {\n",
        "        background-color: #f6cd61;\n",
        "        font-weight: bold;\n",
        "        color: #000;\n",
        "    }\n",
        "    .token.token-ner .ner-label {\n",
        "        color: #9a1f40;\n",
        "        margin: 0px 2px;\n",
        "    }\n",
        "    </style>\"\"\"\n",
        "\n",
        "    if not css_is_load:\n",
        "        display(HTML(css))\n",
        "        css_is_load = True\n",
        "\n",
        "    model, tokenizer, labels = parsbert_ner_load_model(model_name)\n",
        "\n",
        "    if not model or not tokenizer or not labels:\n",
        "        print(not model)\n",
        "        print(tokenizer)\n",
        "        print(labels)\n",
        "        return 'Something wrong has been happened!'\n",
        "\n",
        "    output_predictions = []\n",
        "    for sequence in texts:\n",
        "        sequence = cleanize(sequence)\n",
        "        tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sequence)))\n",
        "        inputs = tokenizer.encode(sequence, return_tensors=\"tf\")\n",
        "        outputs = model(inputs)[0]\n",
        "        predictions = tf.argmax(outputs, axis=2)\n",
        "        predictions = [(token, label_translate[labels[prediction]]) for token, prediction in zip(tokens, predictions[0].numpy())]\n",
        "\n",
        "        if not visualize:\n",
        "            output_predictions.append(predictions)\n",
        "        else:\n",
        "            pred_sequence = []\n",
        "            for token, label in predictions:\n",
        "                if token not in ['[CLS]', '[SEP]']:\n",
        "                    if label:\n",
        "                        pred_sequence.append(\n",
        "                            '<span class=\"token token-ner\">%s<span class=\"ner-label\">%s</span></span>'\n",
        "                            % (token, label))\n",
        "                    else:\n",
        "                        pred_sequence.append(\n",
        "                            '<span class=\"token\">%s</span>'\n",
        "                            % token)\n",
        "\n",
        "            html = '<p class=\"ner-box\">%s</p>' % ' '.join(pred_sequence)\n",
        "            display(HTML(html))\n",
        "\n",
        "    return output_predictions\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7TvM0H2QUhno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'HooshvareLab/bert-base-parsbert-armanner-uncased'\n",
        "x = parsbert_ner(cleanize(text), model, ner_translate, visualize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "VQNHVzE9Kdfo",
        "outputId": "4d160443-67c7-4b93-802f-b2ca988c17e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'translate'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-49e7cffa5242>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'HooshvareLab/bert-base-parsbert-armanner-uncased'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsbert_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleanize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mner_translate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-ba6ec1c41baa>\u001b[0m in \u001b[0;36mcleanize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# clean text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# do some fns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparsbert_ner_load_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hazm/normalizer.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \"\"\"\n\u001b[1;32m    206\u001b[0m         \u001b[0mtranslations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslation_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslation_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persian_style\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'translate'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-V2ht91LZCU",
        "outputId": "ca0f8d40-b460-424d-c066-8cba967c15a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Something wrong has been happened!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cLkTYwdjU-IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "\n",
        "\n",
        "# # def custom_ner_pipeline(model, tokenizer, text):\n",
        "# #     inputs = tokenizer.encode_plus(\n",
        "# #         text,\n",
        "# #         add_special_tokens=True,\n",
        "# #         max_length=512,\n",
        "# #         return_attention_mask=True,\n",
        "# #         return_tensors='pt'\n",
        "# #     )\n",
        "# #     outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "# #     ner_tags = torch.argmax(outputs, dim=1)\n",
        "# #     return ner_tags\n",
        "\n",
        "# # # Sample text\n",
        "# # text = \"شماره سفارش من 123456 است\"\n",
        "\n",
        "\n",
        "# # # Create NER pipeline\n",
        "# # ner_tags = custom_ner_pipeline(model, tokenizer, text)\n",
        "# # print(ner_tags)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # # Perform NER\n",
        "# # entities = nlp_ner(text)\n",
        "# # for entity in entities:\n",
        "# #     print(f\"Entity: {entity['word']}, Label: {entity['entity']}, Score: {entity['score']}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # def custom_ner_pipeline(model, tokenizer, text):\n",
        "# #     inputs = tokenizer.encode_plus(\n",
        "# #         text,\n",
        "# #         add_special_tokens=True,\n",
        "# #         max_length=512,\n",
        "# #         return_attention_mask=True,\n",
        "# #         return_tensors='pt'\n",
        "# #     )\n",
        "# #     outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "# #     ner_tags = torch.argmax(outputs, dim=1)\n",
        "# #     labels = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']  # Define your own label mapping\n",
        "# #     if ner_tags.dim() == 0:  # Check if ner_tags is a scalar tensor\n",
        "# #         entities = [labels[ner_tags.item()]]\n",
        "# #     else:\n",
        "# #         entities = [labels[tag.item()] for tag in ner_tags[0]]\n",
        "# #     return entities\n",
        "\n",
        "# # # Sample text\n",
        "# # text = \"شماره سفارش من 123456 است\"\n",
        "\n",
        "\n",
        "# # # Create NER pipeline\n",
        "# # entities = custom_ner_pipeline(model, tokenizer, text)\n",
        "# # print(entities)\n",
        "\n",
        "\n",
        "# # def custom_ner_pipeline(model, tokenizer, text):\n",
        "# #     inputs = tokenizer.encode_plus(\n",
        "# #         text,\n",
        "# #         add_special_tokens=True,\n",
        "# #         max_length=512,\n",
        "# #         return_attention_mask=True,\n",
        "# #         return_tensors='pt'\n",
        "# #     )\n",
        "# #     outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "# #     ner_tags = torch.argmax(outputs, dim=1)\n",
        "# #     labels = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']  # Define your own label mapping\n",
        "\n",
        "# #     if ner_tags.dim() == 0:  # Check if ner_tags is a scalar tensor\n",
        "# #         entity = labels[ner_tags.item()]\n",
        "# #     else:\n",
        "# #         entity = labels[ner_tags[0].item()]\n",
        "\n",
        "# #     return entity\n",
        "\n",
        "# # # Sample text\n",
        "# # text = \"شماره سفارش من 123456 است\"\n",
        "\n",
        "\n",
        "# # # Create NER pipeline\n",
        "# # entity = custom_ner_pipeline(model, tokenizer, text)\n",
        "# # print(entity)\n",
        "\n",
        "\n",
        "# def custom_ner_pipeline(model, tokenizer, text):\n",
        "#     inputs = tokenizer.encode_plus(\n",
        "#         text,\n",
        "#         add_special_tokens=True,\n",
        "#         max_length=512,\n",
        "#         return_attention_mask=True,\n",
        "#         return_tensors='pt'\n",
        "#     )\n",
        "#     outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "#     ner_tags = torch.argmax(outputs, dim=1)\n",
        "#     labels = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']  # Define your own label mapping\n",
        "\n",
        "#     if ner_tags.dim() == 0:  # Check if ner_tags is a scalar tensor\n",
        "#         entity = labels[ner_tags.item()]\n",
        "#     else:\n",
        "#         entity = labels[ner_tags[0].item()]\n",
        "\n",
        "#     # Extract the entity from the input text\n",
        "#     import re\n",
        "#     entity_text = re.search(r'\\d+', text)\n",
        "#     if entity_text:\n",
        "#         entity_text = entity_text.group()\n",
        "#     else:\n",
        "#         entity_text = ''\n",
        "\n",
        "#     return entity_text\n",
        "\n",
        "# # Sample text\n",
        "# text = \"شماره سفارش من 123456 است\"\n",
        "\n",
        "\n",
        "# # Create NER pipeline\n",
        "# entity = custom_ner_pipeline(model, tokenizer, text)\n",
        "# print(entity)"
      ],
      "metadata": {
        "id": "I_QCt_PFIs5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"شماره سفارش من 123456 است\"\n",
        "# inputs = tokenizer.encode_plus(text, return_tensors='pt', max_length=512, padding='max_length', truncation=True)\n",
        "# outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# logits = outputs.last_hidden_state\n",
        "# logits = F.softmax(logits, dim=-1)\n",
        "\n",
        "# named_entities = []\n",
        "# for i, token in enumerate(inputs['input_ids'][0]):\n",
        "#     if logits[i, 1] > 0.5:  # threshold for named entity recognition\n",
        "#         named_entities.append(tokenizer.decode(token, skip_special_tokens=True))\n",
        "\n",
        "# print(named_entities)\n",
        "\n",
        "\n",
        "# from transformers import pipeline\n",
        "\n",
        "# ner_pipeline = pipeline('ner', model= model, tokenizer= tokenizer)\n",
        "\n",
        "# text = \"شماره سفارش من 123456 است\"\n",
        "# named_entities = ner_pipeline(text)\n",
        "\n",
        "# print(named_entities)\n",
        "\n",
        "# from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained('hooshvare/parsbert-base-uncased-sentiment')\n",
        "#model = AutoModelForSequenceClassification.from_pretrained('hooshvare/parsbert-base-uncased-sentiment')\n",
        "\n",
        "\n",
        "# # Load model directly\n",
        "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "# model = AutoModelForMaskedLM.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "\n",
        "# # Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"ner\", model=\"HooshvareLab/bert-base-parsbert-uncased\")\n",
        "\n",
        "#ner_pipeline = pipeline('ner', model=model, tokenizer=tokenizer)\n",
        "\n",
        "text = \"شماره سفارش من 123456 است\"\n",
        "named_entities = pipe(text)\n",
        "print(named_entities)\n"
      ],
      "metadata": {
        "id": "i_JLEVmzGMWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d1824ec-ad91-456e-f536-636357eae2de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity': 'LABEL_0', 'score': 0.6116472, 'index': 1, 'word': 'شماره', 'start': 0, 'end': 5}, {'entity': 'LABEL_0', 'score': 0.66270363, 'index': 2, 'word': 'سفارش', 'start': 6, 'end': 11}, {'entity': 'LABEL_0', 'score': 0.62693554, 'index': 3, 'word': 'من', 'start': 12, 'end': 14}, {'entity': 'LABEL_0', 'score': 0.7503306, 'index': 4, 'word': '123456', 'start': 15, 'end': 21}, {'entity': 'LABEL_0', 'score': 0.5664961, 'index': 5, 'word': 'است', 'start': 22, 'end': 25}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference\n",
        "\n",
        "Extract Entities: Use the trained model to identify entities in new inputs."
      ],
      "metadata": {
        "id": "wRBGV4sMI9QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entities(model, tokenizer, text):\n",
        "    nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "    return nlp_ner(text)\n",
        "\n",
        "user_query = \"Can you track order 12345?\"\n",
        "entities = extract_entities(model, tokenizer, user_query)\n",
        "print(entities)  # Outputs the entities identified in the text\n"
      ],
      "metadata": {
        "id": "rI8y53DLI-4i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "626ec366-85cf-45d5-9409-e1bcc6ca5bad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ParsBertClassifier' object has no attribute 'config'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d1c2cc30d143>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0muser_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Can you track order 12345?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Outputs the entities identified in the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-d1c2cc30d143>\u001b[0m in \u001b[0;36mextract_entities\u001b[0;34m(model, tokenizer, text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnlp_ner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0muser_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Can you track order 12345?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m         )\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m     \u001b[0mhub_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_commit_hash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0mload_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_class\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1709\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ParsBertClassifier' object has no attribute 'config'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrating Intent Recognition and Entity Extraction\n",
        "After implementing both intent recognition and entity extraction, integrate them to handle user inputs effectively:\n",
        "\n",
        "Predict the Intent: First, determine what the user wants.\n",
        "Extract Entities: Then, identify the key information needed to fulfill the user’s request."
      ],
      "metadata": {
        "id": "7_6Fy4B3JCHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"What's the status of order 12345?\"\n",
        "intent_id = predict_intent(model, tokenizer, user_query)\n",
        "entities = extract_entities(model, tokenizer, user_query)\n",
        "\n",
        "# Example outputs\n",
        "print(f'Intent: {intents[intent_id]}')  # Intent: GetOrderStatus\n",
        "print(f'Entities: {entities}')          # Entities: [{'word': '12345', 'entity': 'B-ORDERID', 'score': 0.99}]"
      ],
      "metadata": {
        "id": "caw_TPBvJQmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "Combining intent recognition and entity extraction allows your chatbot to understand user queries and act accordingly. Hugging Face Transformers and Python offer a robust toolkit for developing these functionalities, ensuring your chatbot can handle a wide range of customer interactions effectively."
      ],
      "metadata": {
        "id": "SD3Ylf5aJpTg"
      }
    }
  ]
}
